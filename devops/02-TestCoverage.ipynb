{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Test Coverage\n\nWriting tests for your application lets you check that the code you wrote works the way you expect. `pytest-cov` is a mature full-featured framework to write your tests efficiently and producting coverage reports. It removes all the overheads for creating tests.\n\nYou should test as much of your code as possible. The closer you get to 100% coverage, the more comfortable you can be that making a change won’t unexpectedly change other behavior. However, 100% coverage doesn’t guarantee that your application doesn’t have bugs. Despite this, code and test coverage is an important tool to use during development.\n\nCode coverage is a measurement of how many lines/blocks of your code are executed while the automated tests are running. With testing, code coverage indicates how well your test set is covering your source code (i.e. to what extent is the source code covered by the set of test cases). Having \"100% code coverage\" doesn't mean everything is tested completely. While it means every line of code is tested, it doesn't mean they are tested under every situation."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Install pytest-cov\n\nThe `azure-pipelines.yml` contains the below snippet to install `pytest-cov` for generating tests and coverage reports.\n\n````\n- script: |\n    pip install pytest-cov\n  displayName: 'Install Test and Coverage Libraries'\n````"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Run Tests and Coverage\n\nThe below task in `azure-pipelines.yml` is the main section that runs tests and generates reports. The `inlineScript` points to the `testing` folder and runs tests located in it. The schema file is passed as an argument and the test reports are generated in `junit-xml` format.\n\n````\n- task: AzureCLI@1\n  displayName: 'Run Tests and Coverage'\n  inputs:\n    azureSubscription: 'serviceConnection'\n    scriptLocation: inlineScript\n    inlineScript: 'py.test devops/code/testing/ --name devops/data_sample/predmain_good_schema.csv --junitxml=reports/test_report.xml'\n````"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Test Code\n\nThe `devops/code/testing` folder contains the different tests that are run. The following are the different types of tests performed:\n\n| Name   |      Description      |\n|----------|-------------|\n| _test_registered_model_ |  Tests if model.pkl is registered |\n| _test_registered_model_tags_ |    Confirms if meaningful tags are used for the experiment   |\n| _test_scoring_image_present_ | Checks if an image is deployed with model.pkl |\n| _test_registered_model_metric_ | Checks if a new model is promoted and if it's accuracy is above 85% |\n| _test_schema_types_ | Checks if the data contains float values |\n| _test_schema_cols_ | Checks schema for the right number of columns |"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Publish Test Results\n\nThis task publishes test results to Azure Pipelines when tests are executed to provide a comprehensive test reporting and analytics experience. \n\nOn each step and job, you can specify the conditions under which the step or job will run. With this step, we will publish test results irrespective of whether a previous task has succeeded or failed. The test report generated using pytest-cov is passed in this task.\n\n````\n- task: PublishTestResults@2\n  inputs:\n    testResultsFiles: 'reports/test_report.xml'\n    testRunTitle: 'ADPM Tests: $(Agent.OS) - $(Build.DefinitionName)'\n  condition: succeededOrFailed()\n````\n\nThe published test results are displayed in the `Tests` tab in the pipeline summary and help you to measure pipeline quality and troubleshoot issues.\n\n![publishedTests](../images/publishedTests.png)\n\nYou will also be able to view the published test results in the `Summary` tab of Azure pipelines portal along with build and deployments information."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise\n\n1. The specific tests run can also be filtered by test name or outcome in the `Tests` tab. Can you change the filters to list all the tests that were run?\n\n2. The log from `Run Tests and Coverage` will indicate code coverage as shown below. Can you explain why only 66% is covered with `test_artifacts.py`?\n\n````\n============================= test session starts ==============================\nplatform linux -- Python 3.6.8, pytest-4.3.1, py-1.8.0, pluggy-0.9.0\nrootdir: /home/vsts/work/1/s, inifile:\nplugins: cov-2.6.1\ncollected 6 items\n\ndevops/code/testing/test_artifacts.py ....                               [ 66%]\ndevops/code/testing/test_data.py ..                                      [100%]\n````\n\n\n3. Can you create another test in `devops/code/testing/test_data.py` to check if the values of the data sample are within a range (min/max or sd)? Perform another build and check to see if the `Tests` tab of the portal got updated.\n\n    ````python\n        === Solution ===\n        def test_schema_types(get_sample):\n            min = -10.0\n            max = 10.0\n            for col in data:\n                try:\n                    val = float(col)\n                    assert min <= val <= max\n                except ValueError:\n                    raise AssertionError\n    ````\n\n4. The pipeline also performs ACI Service Test using the below task. However, this test is not published. Can you integrate this test with `pytest-cov` and publish the result?\n\n````\n- task: AzureCLI@1\n  displayName: 'ACI Service Test'\n  inputs:\n    azureSubscription: 'serviceConnection'\n    scriptLocation: inlineScript\n    inlineScript: 'python devops/code/aci_service_test.py'\n````"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}